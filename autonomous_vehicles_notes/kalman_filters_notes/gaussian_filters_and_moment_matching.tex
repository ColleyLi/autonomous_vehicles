\section{Gaussian Filters and Moment Matching}
\label{gaussian_filters_and_moment_matching}


In the following couple of videos,
we will cover a family of Gaussian filters called
sigma point filters that are based on the moment matching
principle.
Now there are quite a few filters
that are based on this idea.
In principle, the only difference between them
is the method that they use to compute the involved
expectation integrals.
The specific variants that we will cover in this course
are the unscented Kalman filter and the cubature Kalman filter.
Now in this video, we will look at how
we can explain this family of filters using the moment
matching principle.
Now in this course, we mainly consider
models of the following type--
that is, both the motion model function and the measurement
model function are possibly nonlinear functions,
but the noise components are additive and Gaussian.
Now, it's straightforward to generalize these methods
that we are about to study to non-additive Gaussian noise,
but we'll not cover that here.
In many cases, we will also drop the time indices of f and h,
but that's just for notational convenience.
As mentioned in the previous video,
the idea in Gaussian filtering is
to perform filtering using the conventional prediction
and update step of where we approximate the predicted
and updated density as Gaussian densities, like this.
This means that filtering boils down
to finding these four moments.
So we need to find the mean and covariance
of the predicted density, and the mean and covariance
of the updated density.
So the problem that we are facing is,
how can we choose these moments such
that our Gaussian density approximates
these non-Gaussian densities as good as possible?
So let us consider how one can do that.
Suppose for instance that we are given
a non-Gaussian density, p of x.
Now, p of x could, for instance, look something like this
if x is scalar.
Our task is to find x hat and p, such that p of x
is approximately Gaussian with mean xhat
and covariance P. One strategy is
to select xhat and P to match the moments of p of x.
We call this strategy the moment matching principle.
So we ensure that xhat is the mean
of p of x, and that capital P is the covariance of p of x, which
can be expressed like this.
So xhat is this integral here-- so an integral of x times
p of x, dx.
And capital P is this integral here,
where we have x minus xhat times x minus xhat,
transpose, times p of x, dx.
Now note that we can view this covariance here
as an expected value, where we take
the expected value of this function here.
So in order to do moment matching,
we basically need to calculate these two expected
values, which are expressed by these two integrals here.
For the example that we looked at up here,
the mean of the Gaussian would be in the middle
here, so the bell-shaped Gaussian
approximation would look roughly something like this.
So this would be our Gaussian approximation of p of x.
Now, if you think that the moment matching principle seems
a bit arbitrary, there is a theoretical argument
in favor of the principle.
Namely, one can show that moment matching minimizes
the Kullback-Leibler divergence.
Now if you're not familiar with the Kullback-Leibler
divergence, you don't need to learn it for this course.
In this case, it's something that
tells us how similar p of x is to our Gaussian approximation.
So a small Kullback-Leibler divergence
means that the approximation is good,
and a large Kullback-Leibler divergence means that this is
not such a good approximation.
Now among all the moments, xhat and p,
these are the choices that give the smallest
Kullback-Leibler divergence.
Let us see how we can make use of the moment matching
principle to perform the prediction in the prediction
step.
We assume that we are given the updated posterior distribution
at time k minus 1, and in the Gaussian filter case,
we assume that this is a Gaussian.
Further, as we pointed out before,
we assume that the motion model is
a nonlinear function of the state plus some Gaussian noise.
To compute the Gaussian approximation
to the predicted density, we simply
compute the first two moments of the random variable,
xk, given y1, to k minus 1.
So these are the mean, xhat k, given k minus 1,
which is the expected value of this.
To calculate this expected value,
we need to solve the integral of f xk minus 1 times
our Gaussian prior, d xk minus 1.
And that is because qk minus 1 is
0 mean, so it does not contribute
to this expected value here.
Similarly, to compute the covariance
of the predicted distribution, we
take the covariance of xk given y1 to k minus 1.
Now we can separate this into the covariance of qk minus 1,
plus the covariance of f of xk minus 1.
So the covariance of qk minus 1 is just capital Qk minus 1,
and the covariance of f of xk minus 1
is this integral here, where we take
the integral of f of xk minus 1, minus the predicted mean,
times the same thing again, transposed, times this Gaussian
density again.
And we integrate over xk minus 1.
So by doing this, we can perform the prediction step.
The connection to the moment matching principle
is that we have a density, p of xk given y1,
to k minus 1, which we approximate as a Gaussian
distribution with the same mean and the same covariance
as our possibly non-Gaussian predicted density.
So this is approximately Gaussian,
with mean xhat k given k minus 1,
and covariance Pk given k minus 1, where these moments are
selected such that they match the mean and the covariance
of the possibly non-Gaussian predicted density, which
we obtained using these equations here.
Another thing that I would like you to notice
is that both these integrals have a similar form,
and that is that we have a function times a Gaussian
that we take the integral of.
And here, we have a function times a Gaussian
that we take the integral of.
As we will see later, all the integrals we need to solve
can be written on this form.
Now this will come in handy when we
want to find good approximations of these integrals.
To perform the update step using moment matching,
is just a bit more complicated.
We are now given a Gaussian approximation
to the predicted density, and we have a measurement model
with a nonlinear function of our state,
plus some additive Gaussian noise.
The ideal solution, according to the moment matching principle,
is a set xhat k, given k, and pk given k,
to the first two moments of the posterior density--
so density of xk given measurements up to time k.
Unfortunately, it turns out that it's
difficult to compute the first two
moments of this distribution in an efficient manner.
Now that doesn't mean that there are no methods that do that,
but it's just that they are less efficient.
An alternative strategy is to perform moment matching
on the joint distribution of xk and yk.
Now it turns out that this is much easier.
So we approximate xk and yk given
all the previous measurements as a Gaussian
using moment matching.
Once we obtain a Gaussian approximation of this,
we can find the desired density analytically
and nicely using the lemma on conditional Gaussian
distributions that we have looked
at previously in this course.
So the idea is to approximate the joint distribution
of xk and yk as Gaussian.
Now the difficult part is to find the mean
and the covariances here.
Fortunately, we already know the expected value of xk given y1,
to k minus 1, and that is xhat k, given k minus 1.
And its covariance is Pk, k minus 1,
since these are given by the prediction step.
So at least we know these two components here.
Now to find the mean covariance of yk, given y1, to k minus 1,
we make use of the measurement models
that tells us that yk is equal to h of xk plus rk.
This means that the expected value of yk,
given y1, to k minus 1, is the expected value of rk
plus the expected value of h of xk.
But since rk is 0 mean, the expected value
is simply the expected value of h of xk, which
is given by this integral here.
So we take the integral of h of xk times
this Gaussian predicted density here,
and then we integrate over xk.
To compute the covariance of yk, which
is the covariance of these two terms,
we can use that these two are independent.
This means that this covariance here
will decompose into two terms.
First, the covariance of rk, which is capital Rk, and then
the covariance of h of xk, which is this covariance here.
So the covariance of yk, given y1, to k minus 1,
decomposes into one component corresponding
to the measurement noise-- so Rk--
and one component corresponding to the uncertainty in xk, which
is given by this integral here.
So we now know the mean of y and the covariance of y,
so the only thing that remains is to compute cross-covariances
here.
Before we do that, we should note that P of yx
is just P of xy transposed.
So it's enough that we compute one of them,
and then we're done.
By definition, the cross-covariance, Pxy,
is the expected value of xk minus the predicted mean,
times yk minus the predicted measurement, transposed.
From the measurement model, we can
see that yk is h of xk plus rk, where rk is
0 mean, and independent of xk.
This means that the term rk will not contribute to covariance,
so we can simply replace yk with h of xk.
By doing so, the expected value can
be expressed like this integral here,
where we take the integral over this product
where we have replaced yk with h of xk,
times the predicted density of xk,
and then we integrate over xk.
Finally, I would like you to note again
that all the integrals that we need to solve here
have the same form as the previous integrals.
Namely, it's a nonlinear function times a Gaussian,
and it's a nonlinear function times a Gaussian,
and a nonlinear function times a Gaussian.
So that's the only type of integral
that we need to learn how to solve.
Once we managed to do that, we can compute all the moments
here, and then we're almost done.
The only thing left is how to put it all together
to express the updated density.
Suppose that we have managed to compute the first two
moments of the joint distribution of xk and yk.
In that case, we know from the lemma on conditional Gaussian
densities that the conditional density, P of xk,
given yk, and y1, to k minus 1--
now, this here is just y1, to k--
has the expected value, xhat k given
k minus 1, which is equal to the predicted mean,
plus the cross-covariance between x and y,
times the covariance of y inverse,
times yk minus the predicted measurement.
Expressed in the terms of the conventional Kalman filter
equations, we can note that this part here
is the same as the Kalman gain, and this part here
is just innovation.
So this is just the standard form
of the Kalman filter update equation for the estimate.
We also know from the same lemma that the posterior covariance,
Pk given k, is equal to the predicted covariance,
minus the cross-covariance, times Sk inverse,
times the cross-covariance transpose.
Now if we make use of this relation again,
we can also write the whole thing here
as Kalman gain times Sk, times the Kalman gain
transposed, which might be a form that you
recognize a bit better.
As we can see here, update equations
look pretty much like the update equations
in a conventional Kalman filter.
However, there is an important difference,
and that is that all the components here--
so yhat k given k minus 1, Sk, and Pxy--
all need to be approximated.
In order to find these components,
we need to solve integrals on the following form.
So the integral of some nonlinear
function of x times the Gaussian density of x, dx.
And we will therefore dedicate the next video
on techniques of how to solve these integrals.

\subsection{Integrals involved in Gaussian filtering}


In the previous video, we concluded
that we need to solve five integrals of very similar types
in order to perform Gaussian filtering.
In this video, we will look at these integrals
in a bit more detail and give a high level
explanation of common methods to approximate these integrals.
We will use this as an introduction to the filtering
methods that we will present in the next couple of videos.
These filters make use of the so-called sigma point methods
to perform Gaussian filtering.
So in the previous video, we concluded
that in order to derive Gaussian filters using the moment
matching principle, we need to solve integrals on this form.
If we start by considering the prediction step
we need to approximate these two integrals in order
to calculate the predicted mean and the predicted covariance.
As you can see, they both have the same basic form,
where we have a nonlinear function times a Gaussian
density that we take the integral of.
Now in this first integral, we have
f of xk minus 1, which is our nonlinear function.
And in this integral here, we have f of xk minus 1
minus our predicted mean, times the same thing
transposed, which is our nonlinear
function in this case.
Please note that both these integrals here
are expected values, and we can view, for instance,
this integral here as the expected value
of g of x, where x is distributed according
to this Gaussian distribution.
With this in mind, it might be a bit easier
to understand the implications of the integrals here.
So we have an expected value here,
and then we have the covariance here
which can be expressed in terms of the expected value
of this function here.
Once we have computed or approximated these integrals,
we approximate the predicted density using moment matching
as a Gaussian with the mean and covariance
that we get from these expressions here.
Similarly in the update step, we have these three integrals here
that are also on the same form.
All of these take a nonlinear function times a Gaussian,
a nonlinear function times a Gaussian,
and a nonlinear function times a Gaussian
just as we did in the prediction step.
In this case, the integral calculates
the predicted measurement-- the cross-covariance
between the state in our measurement
and the covariance of our measurement,
or the covariance of our innovation, which is the same.
Once we have solved or approximated these components,
we can approximate our posterior as Gaussian
with this mean and this covariance
that we get from using the ordinary Kalman
filter update equations.
To give a concrete example of what these integrals could
represent, we can consider a sensor
that measures the distance and angle to an object.
If the sensor is located at the origin here,
then the distance to the object is simply
the square root of x1 squared plus x2 squared,
and the angle to the object is just arctan x2 over x1.
So for our sensor, we have this measurement model here.
To make things a bit simpler, we assume
that we have no measurement noise, which is perhaps
a bit unrealistic, but it's just for illustration purposes.
If we assume that the object that we are measuring
is Gaussian distributed with mean 2, 2--
which is here-- and this covariance,
then a contour plot of this Gaussian density
could look something like this.
Now let us consider what happens to the distribution
of the measurements if x is distributed like this.
When x is close to its mean, the angle here, theta,
would be roughly pi over 4.
So we take the angle like this.
And the distance, t, is fairly close.
Now if x is out here, we have an angle which is larger,
and the distance grows compared to what we had before.
However, if x is here somewhere, the angle is fairly small--
close to zero-- but the distance grows again.
So by following this argument, we can understand that p of y,
the distribution of the measurement,
has this type of banana shape density, which is clearly not
your typical Gaussian shape.
Now computing and describing p y analytically
is rather complicated.
So in order to compute the expected value of y,
we instead make use of the distribution
of x and the mapping h of x.
So we write the expected value of y as this integral
here, where we have h of x times the Gaussian density of x
with parameters xhat and P, dx.
Now the question is how can we solve this type of integral?
Surely, you're aware of the fact that there are many techniques
to approximate integrals.
In this case, however, the integrals
represent expected values, and for expected values,
there are special techniques that we can use.
A very general and useful technique
is the Monte Carlo method.
The idea is simply to generate independent and identically
distributed samples, x1 to xN from p of x,
and make use of the fact that the expected values
can be approximated by sampling averages in this case.
So the expected value of g of x, where
x is distributed according to p of x,
is approximate 1 over N with a sum of g of xi.
So we evaluate g at all the sample points.
The reason why this is approximately the same
as the expected value is because this is the sample average,
and according to the law of large numbers,
this is asymptotically exact as you increase the sample size,
N. The Monte Carlo method is, in fact,
very simple to use to perform Gaussian filtering,
and as I said, it's asymptotically exact.
But despite all this, it's still seldom used in practice,
and one reason for that might be that you
need to use quite large sample sets in order
to get a good approximation.
As an example of how the Monte Carlo method works,
we can look at the example from the previous slide, where
we have polar measurements.
What we've done here is that we generate samples
from the Gaussian random variable x,
and then map these to our nonlinear function,
h of x, to get the corresponding distances and angles,
which means that we obtained samples from y.
As expected, you can see that the samples of y
follow this banana shape density that we
saw in the previous line.
Now given these samples, it's simple to compute
the sample average, which in this case
is the red cross here.

And if our sample size is sufficiently large,
this will be a very good approximation
of the expected value of y.
The covariance of y can be approximated
in a similar manner by setting g of x
equal to h of x, minus yhat times itself transpose,
where yhat is the mean of y that we calculated here.
So that is the Monte Carlo method.
Now let's look at another set of methods
that feels a bit similar, but have much less
computational complexity, but also gives
less accurate results.
Nonetheless, these methods are far more
used to do Gaussian filtering than the Monte Carlo method,
and these are the sigma point methods.
All of the so-called sigma point methods
that we will be studying in the next couple of videos
makes use of something called stochastic decoupling.
It turns out that it's easy to design integration methods when
the elements in the random vector are independent.
By using stochastic decoupling, we
can express an integral of a general Gaussian
random variable in terms of an integral
with respect to a Gaussian random variable, which
is 0 mean, and has the identity matrix as a covariance matrix.
The argumentation goes like follows.
Suppose the square root of P is a matrix
such that the square root of P times
itself transposed is P. One way to find a square root
is to use the so-called Cholesky decomposition which
we get in MATLAB if we simply write Chol P, and add lower.
This will give us a lower triangular matrix
which has all zeros on the upper triangle,
and then only values on the diagonal
and on the lower triangle, like this.
Now we should perhaps note that the square root of a matrix
is not unique.
There are many different solutions for which this holds,
and depending on which you use, you
will get slightly different results.
So keep this in mind later when you
use these methods in your own filter implementations.
Suppose now that we have a random vector, xi,
which is Gaussian with 0 mean, and the identity
matrix as a covariance matrix.
As a minor remark, I would like to point out
that we will later learn how to compute integrals with respect
to this type of densities.
If we then define a new random vector, x,
which is equal to xhat plus the square root of P,
times xi, then it's easy to see that the expected value of x is
just xhat since xi is 0 mean.
And the covariance of x is square root of P times I,
times the square root of P, transposed.
And this is just P.
So x here is a Gaussian random variable
with mean xhat and covariance P. And by writing it like this,
we can describe any Gaussian random variable
in terms of this simple Gaussian random variable, xi
that has 0 mean and the identity matrix as a covariance matrix.
In terms of our integrals used to compute our expected values,
this means if we're interested in taking
in the expected value of g of x, which is distributed according
to this Gaussian distribution, we can instead
take the expected value of g of this variable
here, where the expectation is taking over xi instead,
which is 0 mean and has the identity matrix as covariance.
The only thing that we have done here
is that we have performed a change of variable
in the integrations.
We change from x to xi using this relationship here.
The conclusion from all this is that it's
sufficient to be able to compute integrals on this form.
So we have the integral of a general function, g of xi,
which in the example here would be all of this.
So we call this g of xi.
So we take the integral of this type of function
times this density, which is 0 mean and has the identity
matrix as a covariance.
And if we can solve this, then we can do Gaussian filtering.
And I would argue that this is easier to solve
than of these integrals, where we have
general Gaussian distributions.
Now I can understand if, at the moment,
these results may look rather dry and boring,
but we will soon show techniques to compute these integrals,
and thanks to the results presented here,
we will be ready to make use of them to perform filtering.
So now finally to the sigma point methods.
So apart from the Monte Carlo methods,
all integration methods covered here
can be viewed as sigma point methods.
The idea is to approximate the expected value
of g of x as a weighted sum of g evaluated
at a set of sigma points, Xi, and weighted
by its own weight, Wi.
But at first glance, this might seem very similar to the Monte
Carlo approximation.
A key difference is the sigma points here
are selected deterministically, and normally, there
are not so many of them.
For instance, if we return to the previous example
with the sensor measuring the angle,
and the distance to an object, and this banana shaped density.
If we try to compute the expected value of y using
a sigma point method called an unscented transform,
we would only select five sigma points which would result
in five different values of g of x-- here,
marked with the yellow x:es.
If we then use these values of g of Xi and the weighting factors
to compute the expected value, we
get this approximation here which is not too far off
from the true value.
Now there is not just one sigma point method,
but there are many sigma point methods, each
with its own peculiar name.
So we have the unscented transform,
the cubature rule, Gauss-Hermite quadrature, Gaussian Process
quadrature, marginalized transform, and so on.
Now all of these have also been used in Gaussian filtering,
and there are filters with equally peculiar names,
as they are named after corresponding integration
methods.
So we have the unscented Kalman filter, the cubature Kalman
filter, the Gauss-Hermite Kalman filter, Gaussian Process Kalman
filter, marginalized Kalman filter,
and there are several more.
So it's quite simple--
you invent a new integration rule,
and you have a new filter.
It also means that you can learn and understand
several of these filters at the same time.
In the next videos, we will present two integration rules,
and explain how these can be used for Gaussian filtering.
The integration rules are the unscented transform,
which is perhaps the most commonly used sigma point
method, perhaps because it was the first one to be promoted.
Now there are variants of the unscented transform that
has several tuning parameters, which
allow us to try to tailor its behavior
to our specific problem, but it also makes it harder
for us to work with it.
The second integration rule is called the cubature rule, which
is more recent than the unscented transform
and is very similar, but even simpler.
Compared to the unscented transform,
it uses one less point and it has no tuning parameters,
which in turn makes it simple and quite robust.
So in the next couple of videos, we
will explain how we can use these to do Gaussian filtering.

\section{Questions}

\begin{enumerate}
\item Which of the following statements are true?

\begin{enumerate}
\item Our choice of density parametrization will both effect the performance of our filter and the computational complexity. 
\item In real-time applications, having a recursive filter means that our compute time for producing a new estimate is more or less constant with each new measurement. 
\item In nonlinear filtering, a recursive algorithm will always find a better or equally good approximation of the true posterior as a non-recursive filter. 
\item A prerequisite for a recursive filter is that the prior and posterior density in each recursion can be described using the same density parametrization. 
\end{enumerate}

\item  In Gaussian filtering using moment matching we need to solve several different integrals (two during the prediction and three during 

\item  Which of the following statements are true?

\begin{enumerate}
\item The Monte Carlo method approximates expected values by sample averages
\item A $\sigma$-point method makes use of a small number of weighted random samples. 
\item Stochastic decoupling is about rewriting an integral with respect to a vector of correlated random variables as an integral with respect 
to a vector of independent random variables that have zero mean and unit variance. 
\item The Gauss-Hermite Kalman filter, the Unscented Kalman filter and the Cubature Kalman filter are all Gaussian filters and they only differ in how they approximate the involved integrals. 
\end{enumerate}
\end{enumerate}


