\section{Hardware for Self-driving Cars}
\label{hardware_for_self_driving_cars}
In this section, we will  discuss many details about the hardware used in
today's self-driving cars. In particular, we will cover the various
sensors that can be used for perception. The basics of designing sensor
configurations for self-driving cars.

 In this video, we will define sensors, and discuss the various types of sensors
available for the task of perception. And we will then discuss the self-driving
car hardware available nowadays. 

\subsection{Sensors}
\label{sensors}

Let's begin by talking about sensors. Even the best perception algorithms
are limited by the quality of their sensor data. And careful selection of sensors
can go a long way to simplifying the self-driving perception task. Let's try to give a definition of
what a sensor is.


\begin{framed}
\theoremstyle{definition}
\begin{definition}{\textbf{What is a sensor? }}

For our purposes, a sensor is any device that measures or
detects some property of the environment, or changes to that property over time.
\end{definition}
\end{framed}


Sensors are broadly categorized into two types, depending on what property they record. If they record a property of
the environment they are {\textbf{exteroceptive}}. Extero means outside, or from the surroundings. On the other hand, if the sensors
record a property of the ego vehicle, they are {\textbf{proprioceptive}}. Proprios means internal, or one's own. Let's start by discussing
common exteroceptive sensors. 



We start with the most common and widely used sensor in autonomous driving, the camera. Cameras are a passive, light-collecting
sensor that are great at capturing rich, detailed information about a scene. In fact, some groups believe that the
camera is the only sensor truly required for self-driving. But state of the art performance is
not yet possible with vision alone. While talking about cameras, we usually tend to talk about three
important comparison metrics. We select cameras in terms:


\begin{itemize}
\item resolution
\item field of view or FOV
\item dynamic range
\end{itemize}

The resolution is the number of pixels that create the image. So it's a way of specifying
the quality of the image.  The field of view is defined by the horizontal and vertical angular extent that is visible to the camera, and can be
varied through lens selection and zoom. The dynamic range of the camera is the difference between the darkest and the lightest tones in an image. High dynamic range is critical for
self-driving vehicles due to the highly variable lighting conditions encountered
while driving especially at night. 

There is an important trade off cameras and lens selection, that lies between the choice of
field of view and resolution. Wider FOV permits a lager
viewing region in the environment, but fewer pixels that absorb light from one particular object. As the FOV increases, we need to increase resolution to still be
able to perceive with the same quality, the various kinds of information we may encounter. 
Other properties of cameras that affect perception exist as well, such as focal length, depth of field and frame rate. 
 
The combination of two cameras with overlapping fields of view and aligned image planes is
called the stereo camera. Stereo cameras allow depth estimation
from synchronized image pairs. Pixel values from image can be
matched to the other image producing a disparity map of the scene. This disparity can then be used
to estimate depth at each pixel. 


Next we have LIDAR which stands for light detection and ranging sensor. LIDAR sensing involves shooting
light beams into the environment and measuring the reflected return. By measuring the amount of returned
light and time of flight of the beam. Both in intensity in range to
the reflecting object can be estimated. LIDAR usually include a spinning element
with multiple stacked light sources and outputs a three dimensional
point cloud map, which is great for assessing scene geometry. Because it is an active sensor
with it's own light sources, LIDAR are not effected by
the environments lighting. So LIDAR do not face the same challenges
as cameras when operating in poor or variable lighting conditions. Let's discuss the important comparison
metrics for selecting LIDAR.

\begin{itemize}

\item The first is the number of sources it contains with 8, 16, 32, and 64 being common sizes. 
\item the second is the points per second it can collect. The faster the point collection, the more
detailed the 3D point cloud can be. 
\end{itemize}

Another characteristic is the rotation rate. The higher this rate, the faster
the 3D point clouds are updated. Detection range is also important, and is dictated by the power
output of the light source. And finally, we have the field of view,
which once again, is the angular extent
visible to the LIDAR sensor. 

Finally, we should also mention the new
LIDAR types that are currently emerging. High-resolution, solid-state LIDAR. Without a rotational component
of the typical LIDARs, these sensors stand to become
extremely low-cost and reliable. Thanks to being implemented
entirely in silicon. HD solid-state LIDAR
are still a work in progress. But definitely something exciting for
the future of affordable self-driving. 


Our next sensor is RADAR, which stands for radio detection and ranging. RADAR sensors have been
around longer than LIDAR and robustly detect large
objects in the environment. They are particularly useful in adverse
weather as they are mostly unaffected by precipitation. Let's discuss some of the comparison
metrics for selecting RADAR. RADAR are selected based on

\begin{itemize}
\item detection range
\item field of view, 
\item the position and speed measurement accuracy. 
\end{itemize}

RADARs are also typically available as either having a wide angular field of view but short range. 
Or having a narrow FOV but a longer range. 

The next sensor we are going to discuss are ultrasonics or sonars. Originally so named for
sound navigation and ranging. Which measure range using sound waves. Sonars are sensors that are short
range and inexpensive ranging devices. This makes them good for parking scenarios, where the ego-vehicle needs to make
movements very close to other cars. Another great thing about sonar is that they are low-cost. Moreover, just like RADAR and LIDAR,
they are unaffected by lighting and precipitation conditions. A sonar sensor is selected based
on a few key metrics itemized next. 

\begin{itemize}
\item The maximum range they can measure
\item The the detection FOV
\item The cost
\end{itemize}

\subsubsection{Proprioceptive sensors}

Now let's discuss the proprioceptive sensors, the sensors that sense ego properties. The most common ones here
are:

\begin{itemize}
\item Global Navigation Satellite Systems, GNSS for short, such as GPS or Galileo
\item Inertial Measurement Units or IMU's
\item Wheel odometers
\end{itemize}

GNSS receivers are used to measure ego vehicle position, velocity, and sometimes heading. The accuracy depends a lot on
the actual positioning methods and the corrections used. Apart from these, the IMU also
measures the angular rotation rate, accelerations of the ego vehicle, and
the combined measurements can be used to estimate the 3D orientation
of the vehicle. Where heading is the most important for
vehicle control. Finally, we have wheel odometry sensors. This sensor tracks the wheel
rates of rotation, and uses these to estimate the speed and
heading rate of change of the ego car. This is the same sensor that tracks
the mileage on your vehicle. 



In summary, the major sensors used nowadays for autonomous driving perception
include cameras, RADAR, LIDAR, sonar, GNSS, IMUs,
and wheel odometry modules. These sensors have many
characteristics that can vary wildly, including resolution,
detection range, and FOV. 

Selecting an appropriate sensor configuration for a self-driving car is not trivial. Here's a simple graphic that
shows each of the sensors and where they usually go on a car. 


We will revisit this chart again in the
next video, when we discuss how to select sensor configurations to achieve
a particular operational design domain. 


\section{Computing Hardware}

Finally, let's discuss a little bit about the computing hardware most commonly used in  self-driving cars at the time of writing. The most crucial part
is the computing brain, the main decision making unit of the car. It takes in all sensor data and outputs
the commands needed to drive the vehicle. Most companies prefer to design their own
computing systems that match the specific requirements of their sensors and
algorithms. Some hardware options exist, however, that can handle self-driving
computing loads out of the box. 

The most common examples would be Nvidia's Drive PX and Intel \& Mobileye's EyeQ. Any computing brain for self-driving needs
both serial and parallel compute modules. Particularly for image and LIDAR processing to do segmentation, object detection, and mapping. 
For these we employ GPUs, FPGAs and custom ASICs (Application Specific Integrated Circuit), which are specialized hardware to
do a specific type of computation. 

For example, the drive PX units include multiple GPUs. The EyeQs have FPGAs both to
accelerate parallalizable compute tasks, such as image processing or
neural network inference. 

Finally, a quick comment about synchronization. Because we want to make driving
decisions based on a coherent picture of the road scene. It is essential to correctly synchronize
the different modules in the system, and serve a common clock. Fortunately, GPS relies on extremely
accurate timing to function, and as such can act as an appropriate
reference clock when available. Regardless, sensor measurements must be
timestamped with consistent times for sensor fusion to function correctly. Let's summarize. In this video,
we learned about sensors and their different types based
on what they measure. 


