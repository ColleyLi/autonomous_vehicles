\chapter{Gradient-Based Learning}
\label{gradient_based_learning}

Neural networks are a method to approximate arbitrary functions. These functions are typically
nonlinear. This nonlinearity  causes most interesting loss functions to become nonconvex.

This means that training a  NN requires an  iterative, gradient-based optimizers that merely drive the cost
function to a very low value, rather than the linear equation solvers used to train
linear regression models or the convex optimization algorithms with global convergence guarantees used to train logistic regression or SVMs. 

Convex optimization converges starting from any initial parameters (in theory—in practice it is robust
but can encounter numerical problems). Stochastic gradient descent applied to
nonconvex loss functions has no such convergence guarantee and is sensitive to the
values of the initial parameters. 

\begin{framed}
\begin{remark}
For feedforward neural networks, it is important to
initialize all weights to small random values. The biases may be initialized to zero
or to small positive values. 
\end{remark}
\end{framed}

For the moment, it suffices to understand that the training algorithm
is almost always based on using the gradient to descend the cost function in one
way or another. 


We can of course train models such as linear regression and support vector
machines with gradient descent too, and in fact this is common when the training
set is extremely large. From this point of view, training a neural network is not
much different from training any other model. Computing the gradient is slightly
more complicated for a neural network but can still be done effciently and exactly.

As with other machine learning models, to apply gradient-based learning we
must choose a cost function, and we must choose how to represent the output of
the model. We now revisit these design considerations with special emphasis on
the neural networks scenario.

\subsection{Cost Functions}

An important aspect of the design of a deep neural network is the choice of the
cost function. Fortunately, the cost functions for neural networks are more or less
the same as those for other parametric models e.g. linear models.

Thus, in most cases, the paramemtric model defines a distribution $p(\mathbf{y} | \mathbf{x}; \boldsymbol{\theta})$ and we simply
use the principle of maximum likelihood. This means we use the
cross-entropy between the training data and the model’s predictions as the cost function.


\begin{framed}
\begin{remark}{\textbf{Principle of Maximum Likelihood}}
\end{remark}
\end{framed} 

Sometimes, we take a simpler approach, where rather than predicting a complete
probability distribution over $\mathbf{y}$, we merely predict some statistic of
it  conditioned on $\mathbf{x}$. Specialized loss functions enable us to train a predictor of these estimates.

The total cost function used to train a neural network will often combine one
of the primary cost functions described here with a regularization term. 
The weight decay approach used for linear models is also directly
applicable to deep neural networks and is among the most popular regularization strategies.


\subsubsection{Learning Conditional Distributions with Maximum Likelihood}

Most modern neural networks are trained using maximum likelihood. This means
that the cost function is simply the negative log-likelihood, equivalently described
as the cross-entropy between the training data and the model distribution. This
cost function is given by

\begin{itemize}
J(\boldsymbol{\theta}) = - E[log p_{model}(\mathbf{y}|\mathbf{x})], \mathbf{x}, \mathbf{y}\sim \hat{p}_{data}
\end{itemize}

The speciﬁc form of the cost function changes from model to model, depending
on the speciﬁc form of $log p_{model}$ . The expansion of the above equation typically
yields some terms that do not depend on the model parameters and may be discarded.


\begin{framed}
\begin{remark}{\textbf{Why Take The Log?}}

One recurring theme throughout neural network design is that the gradient of
the cost function must be large and predictable enough to serve as a good guide
for the learning algorithm. Functions that saturate (become very ﬂat) undermine
this objective because they make the gradient become very small. In many cases
this happens because the activation functions used to produce the output of the
hidden units or the output units saturate. The negative log-likelihood helps to
avoid this problem for many models. Several output units involve an
exponential function
that can saturate when its argument is very negative. The log
function in the negative log-likelihood cost function undoes the
exponential  of some output units.
\end{remark}
\end{framed}


\begin{framed}
\begin{remark}

One unusual property of the cross-entropy cost used to perform maximum
likelihood estimation is that it usually does not have a minimum value when applied
to the models commonly used in practice. For discrete output variables, most
models are parametrized in such a way that they cannot represent a probability
of zero or one, but can come arbitrarily close to doing so. Logistic regression
is an example of such a model. For real-valued output variables, if the model
can control the density of the output distribution (for example, by learning the
variance parameter of a Gaussian output distribution) then it becomes possible
to assign extremely high density to the correct training set outputs, resulting in
cross-entropy approaching negative inﬁnity.

\end{remark}
\end{framed}
