\section{Visual Depth Perception}
\label{visual_depth_perception}

Self-driving cars require accurate depth perception for the safe operation of
our autonomous vehicles. If we do not know how far away the cars are in front of us, how can you avoid them while driving? 
Lidar and Radar sensors are usually thought of as the primary 3D sensors
available for perception tasks. However, we can get depth information from two or more cameras using
multi-view geometry. Specifically, we will be describing the process of getting depth from two axis aligned cameras a setup known as
the stereo cameras. 

In this section, we will cover the geometry of the stereo sensor
as well as how to derive the 3D coordinates of a point given its projection onto two images
of the stereo sensor. 

\begin{framed}
\begin{remark}{\textbf{Some History}}

Stereopsis, the process of stereo vision, was first described by
Charles Wheatstone back in 1838. He recognized that because each eye views
the visual world from a slightly different
horizontal position that each eye's image
differs from the other. Objects at different
distances from the eye project images into
the two eyes that differ in their horizontal position
giving depth cues of horizontal disparity that are also known as
binocular disparity. However, historical evidence suggests that stereopsis was discovered
much earlier than this. In fact some drawings by Leonardo da Vinci depict accurate geometry of
depth through stereopsis. Up to the 19th century, the phenomenon of stereopsis was primarily used for entertainment. 
Anaglyphs were used to provide a stereoscopic 3D effect when viewed with 2D color glass, where each lens employs different chromatically
opposite colors, usually red and cyan. Nowadays, we use stereopsis with complex algorithms to derive depth from two images using a similar concept to
Da Vincis drawings. 
\end{remark}
\end{framed}


Let us now delve into the geometry of a stereo sensor. A stereo sensor is
usually created by two cameras with parallel optical axes. To simplify
the problem even more, most manufacturers align the cameras in 3D space so that the two image
planes are aligned with only an offset in the $x$-axis. Given a known rotation
and translation between the two cameras
and a known projection of a point $\mathbf{O}$ in 3D to
the two camera frames resulting in pixel locations
$OL$ and $OR$ respectively, we can formulate
the necessary equations to compute the 3D coordinates
of the point $\mathbf{O}$. In order to make our computation easier, we will state some assumptions. First, we assume
that the two cameras used to construct the stereo
sensors are identical. Second, we will assume that while manufacturing the stereo sensor, we tried as hard as
possible to keep the two cameras optical axes aligned. Let's now define
some important parameters of the stereo sensor. The focal length $f$ is the distance between
the camera center and the image plane. Second, the baseline is
defined as the distance along the shared $x$-axis between the left and
right camera centers. By defining a baseline to represent the transformation between 
the two camera coordinate frames, we are assuming that the rotation matrix is identity and there is only a
non-zero $x$ component in the translation vector. The $\mathbf{R}$ and $\mathbf{T}$
transformation therefore boils down to a single baseline parameter $b$. 

Before proceeding, we will project the previous figure to bird's eye view for
easier visualization. Now, let's define the quantities we would like to compute. We want to compute the
$x$ and $z$ coordinates of the point $\mathbf{O}$ with respect
to the left camera frame. The $y$ coordinate can be estimated easily after the $x$ and $z$
coordinates are computed. Remember, we are given the baseline, focal length, and the coordinates
of the projection of the point $\mathbf{O}$ onto the left
and right image planes. We can see two similar triangles formed by the left
camera measurement as follows. The triangle formed by the depth $z$ and the position $x$ is similar to the triangle formed
by the focal length $f$ and the left measurement $x$ component $x_l$. From this similarity we
can construct the equation 


\begin{equation}
\frac{Z}{f} = \frac{X}{x_L}
\end{equation}

The same can be done for the right measurements but with the offset for
the baseline included. In this case, the two triangles
are defined by:

\begin{equation}
\frac{Z}{f} = \frac{X-b}{x_R}
\end{equation}

Similarly, we can get a second equation relating $Z$ to $X$ via the right camera
parameters in measurements. 


From these two equations, we can now derive the 3D coordinates of the point $\mathbf{O}$. We define the disparity $d$ to
be the difference between the image coordinates of the same pixel in the
left and right images. We can easily transform between image and pixel coordinates using the $X$ and $Y$ offsets $u_0$ and $v_0$. We then use
the two equations from the similar triangle relations to solve for the value of $Z$ as follows. From there we use
the value of $z$ to compute $X$ with
the following expression. 

\begin{equation}
X = \frac{Zx_L}{f} 
\end{equation}

Finally, we can repeat the process in
the $Y$  direction with the same derivation to arrive at the following
expression for $Y$. 

\begin{equation}
Y = \frac{Zy_L}{f} 
\end{equation}

The three components of the point position are now explicitly available
from the two sets of pixel measurements available to us. Now that we have established
the equations needed for 3D coordinate computation from the stereo sensor, two problems arise to be able to perform this computation. 
First, we need to compute the focal length baseline and $x$ and $y$ offsets. That is, we need to calibrate
the stereo camera system. Second, we need to find
the correspondence between each left and right
image pixel pair to be able to compute
their disparity. Fortunately, the calibration problem can be solved using stereo
camera calibration. This is an extension of the monocular process we
discussed in section \ref{camera_calibration}, for which well-established
implementations are available. The correspondence
problem however, requires specialized algorithms
to efficiently perform the matching and compute the disparity between left and right image pixels, which we'll discuss
further in the next video. The output depth from stereopsis suffers from some limitations particularly as points move further away
from the stereo camera. However, given a good disparity estimation algorithm, the output is still useful for self-driving cars as a dense
source of depth information and closer range which exceeds the density we can get from common Lidar sensors. 

\section{Summary}

To summarize this section,  we discussed  the equations required to estimate 3D
coordinates of a pixel given the geometric
transformation between the two cameras sensors and
the disparity between pixels. In the next section we will learn more about disparity generating algorithms and show
full examples on how to compute that disparity from a stereo image pair using Python and OpenCV.

\section{Questions}

\section{Assignements}