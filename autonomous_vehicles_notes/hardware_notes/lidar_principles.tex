\section{LiDAR Principles}
\label{lidar_principles}

In this section, we will be talking about LIDAR, or light detection and ranging sensors. LIDAR has been an enabling technology
for self-driving cars because it can see in all directions and is able to provide very accurate range information. 
In fact, with few exceptions, most self-driving cars on the road today are equipped with some type of LIDAR sensor. 
In this section, we will learn about the operating principles of LIDAR sensors, basic sensor models used to work with LIDAR data and LIDAR point clouds, different kinds of transformation
operations applied to point clouds, and how we can use LIDAR to localize a self-driving car using a technique
called point cloud registration. 

Concretely, in this section, we will explore how a LIDAR works and take a look at sensor models
for 2D and 3D LIDARs. We will also describe the sources of measurement noise and
errors for these sensors. 

\subsection{Introduction}

If you've ever seen
a self-driving car like the Waymo vehicle or an Uber car, you've probably noticed something
spinning on the roof of the car, see Figure \ref{lidar_1}. That something is a LIDAR, or light detection and ranging sensor, and its job is to provide detailed 3D scans of
the environment around the vehicle. 

\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.280]{img/hardware/lidar_1.jpeg}
\end{center}
\caption{Various types of LiDAR sensors.}
\label{lidar_1}
\end{figure}


In fact, LIDAR is one of the most common sensors used on self-driving cars and many other kinds of mobile robots. 
LIDARs come in many different shapes and sizes, and can measure the distances to a single point, 
a 2D slice of the world, or perform a full 3D scan. Some of the most popular models used today are manufactured by firms such
as Velodyne in California, Hokuyo in Japan, and SICK in Germany, see Figure \ref{lidar_1}. In this section, we will mainly focus on the Velodyne sensors as
our example of choice, but the basic techniques applied
to other types of LIDARs as well. 


\begin{framed}
\theoremstyle{remark}
\begin{remark}{\textbf{Some History}}

LIDAR was first introduced in the 1960s, not long after the invention
of the laser itself. The first group to use
LIDAR were meteorologists at the US National Center
for Atmospheric Research, who deployed LIDAR to measure
the height of cloud ceilings. These ground-based
celiometers are still in use today not only to
measure water clouds, but also to detect volcanic ash
and air pollution. Airborne LIDAR sensors are
commonly used today to survey and map the earth's
surface for agriculture, geology, military, and other uses. But the application that
first brought LIDAR into the public
consciousness was Apollo 15, the fourth manned mission
to land on the moon, and the first to use a laser altimeter
to map the surface of the moon. 
\end{remark}
\end{framed}

\subsection{LiDAR operating principles}

So, we've seen that LIDAR can be used to measure distances and create
a certain type of map, but how did they actually work, and how can we use them
onboard a self-driving car? To build a basic LIDAR in one dimension, you need three components:

\begin{itemize}
\item A laser
\item A photodetector
\item A very precise stopwatch
\end{itemize}

The laser first emits a short pulse of light usually in the near infrared frequency band
along some known ray direction. At the same time,
the stopwatch begins counting. The laser pulse travels
outwards from the sensor at the speed of light and
hits a distant target. Maybe another vehicle in
front of us on the road or a stationary object like
a stop sign or a building. As long as the surface of the target
is not too polished or shiny, the laser pulse will scatter off
the surface in all directions, and some of that reflected light will travel back along
the original ray direction. The photodetector catches that return
pulse and the stopwatch tells you how much time has passed between when the pulse first went out
and when it came back. That time is called the \textbf{round-trip} time. 

Further, we know the speed of light, which is a bit less than 300
million meters per second. So, we can multiply the speed of
light by the round-trip time to determine the total round trip distance
traveled by the laser pulse Figure \ref{lidar_3}. 

\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.280]{img/hardware/lidar_3.jpeg}
\end{center}
\caption{Calculate distance with LiDAR.}
\label{lidar_3}
\end{figure}

Since light travels much faster than cars, it's a good approximation to think
of the LIDAR and the target as being effectively stationary
during the few nanoseconds that it takes for all of this to happen. That means that the distance from
the LiDAR to the target is simply half of the round-trip distance
we just calculated. This technique is called
\textbf{time-of-flight ranging}. Although it's not the only way
to build a LiDAR, it is a very common method
that also gets used with other types of ranging sensors like radar and sonar. 
It is worth mentioning that the photodetector also
tells you the intensity of the return pulse relative to the
intensity of the pulse that was emitted. This intensity information is less
commonly used for self-driving, but it provides some extra information
about the geometry of the environment and the material
the beam is reflecting off of. 

It is possible to create 2D images from LiDAR intensity data that
you can then use the same computer vision algorithms you'll
learn about in the next course. Since LiDAR is its own light source, it actually provides a way for
self-driving cars to see in the dark. 

We know how to measure a single distance to a single point using a laser, a photodetector, a stopwatch, and
the time-of-flight equation, but obviously it's not enough to stay laser focused on a single point ahead. 
How do we use this technique to measure a whole bunch of distances in 2D or in 3D?  The trick is to build
a rotating mirror into the LiDAR that directs the emitted pulses
along different directions. As the mirror rotates, you can measure distances at points
in a 2D slice around the sensor. If you then add an up
and down nodding motion to the mirror along with the rotation, you can use the same principle
to create a scan in 3D. For Velodyne type LiDARs, where the mirror rotates
along the entire sensor body, it's much harder to use
a knotting motion to make a 3D scan. 

Instead, these sensors will actually create multiple 2D scan lines from a series of individual lasers spaced at
fixed angular intervals, which effectively lets you paint the world with horizontal
stripes of laser light. Figure \ref{lidar_4} shows an example of
a typical raw LiDAR stream from a Velodyne sensor
attached to the roof of a car. 

\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.280]{img/hardware/lidar_4.jpeg}
\end{center}
\caption{Typical raw LiDAR stream.}
\label{lidar_4}
\end{figure}

The black hole in the middle is a blind spot where the sensor
itself is located, and the concentric circles
spreading outward from there are the individual scan lines produced
by the rotating Velodyne sensor. Each point in the scan is colored by
the intensity of the return signal. The entire collection of points in the 3D scan is called a point cloud. 

Now typically, LiDARs measure the position of points in 3D using
spherical coordinates, range or radial distance from
the center origin to the 3D point, elevation angle measured up
from the sensors $XY$ plane, and azimuth angle, measured
counterclockwise from the sensors $x$-axis. This makes sense because
the azimuth and elevation angles tell you the direction
of the laser pulse, and the range tells you how far in that direction
the target point is located. The azimuth and elevation angles
are measured using encoders that tell you
the orientation of the mirror, and the range is measured using the time
of flight as we've seen before. 


\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.280]{img/hardware/lidar_5.jpeg}
\end{center}
\caption{Typical raw LiDAR stream.}
\label{lidar_5}
\end{figure}

\begin{framed}
\theoremstyle{remark}
\begin{remark}{}

For Velodyne type LIDARs, the elevation angle is fixed
for a given scan line.
\end{remark}
\end{framed}

Now, suppose we want to
determine the cartesian $XYZ$ coordinates of our scanned point
in the sensor frame, which is something we often
want to do when we are combining multiple LiDAR scans into a map. To convert from spherical
to Cartesian coordinates, we use the same formulas you would havve encountered in your mechanics classes. 

\begin{equation}
\begin{bmatrix}
x \\
y \\
z 
\end{bmatrix} = \mathbf{h}^{-1}(r, \alpha, \epsilon) = 
\begin{bmatrix}
r\cos(\alpha)\cos(\epsilon) \\
r\sin(\alpha)\cos(\epsilon) \\
\sin(\epsilon) 
\end{bmatrix} 
\label{lidar_inverse_sensor_model}
\end{equation}

This gives us an inverse sensor model. We say this is the inverse model because our actual measurements are
given in spherical coordinates, and we are trying to
reconstruct the Cartesian coordinates at the points
that gave rise to them. 

\begin{framed}
\theoremstyle{remark}
\begin{remark}{}

We can obviously get the forward model from the inverse model by using

\begin{equation}
\begin{bmatrix}
r \\
\alpha \\
\epsilon 
\end{bmatrix} = \mathbf{h}(x, y, z) = 
\begin{bmatrix}
\sqrt{x^2 + y^2 + z^2} \\
\arctan(\frac{x}{y}) \\
\arcsin(\frac{z}{\sqrt{x^2 + y^2 + z^2}} 
\end{bmatrix} 
\end{equation}

This is our forward sensor
model for a 3D LiDAR, which given a set of Cartesian coordinates defines what
the sensor would actually report.
\end{remark}
\end{framed}

Note that we have not said anything
about measurement noise yet. Most of the time the
self-driving cars we're working with use 3D LIDAR sensors
like the Velodyne, but sometimes you might want
to use a 2D LIDAR on its own, whether for detecting obstacles
or for state estimation in more structured environments
such as parking garages. Some cars have multiple 2D LiDARs strategically placed to
act as a single 3D LiDAR, covering different areas with a greater lesser density of measurements. 


\begin{framed}
\theoremstyle{remark}
\begin{remark}{\textbf{2D LiDAR}}

For 2D LIDARs, we use exactly the same forward and inverse sensor models. However, the elevation angle
enhance the $z$ component of the 3D point in the sensor frame are both zero. In other words, all of
our measurements are confined to the $XY$ plane of the sensor, and our spherical
coordinates collapsed to the familiar 2D polar coordinates. Figure \ref{lidar_6} summarizes the 2D case.
\end{remark}
\end{framed}

\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.280]{img/hardware/lidar_6.jpeg}
\end{center}
\caption{2D LiDAR.}
\label{lidar_6}
\end{figure}

\subsection{Measurement noise}

We have seen now how to convert between spherical coordinates measured by the sensor and the cartesian coordinates that 
we will typically be interested in for state estimation, but what about measurement noise? For LiDAR sensors, there are several important sources
of noise to consider. First, there is uncertainty in the exact time of arrival of the reflected signal, which comes from the fact that
the stopwatch we use to compute the time of flight necessarily
has a limited resolution. Similarly, there is uncertainty in the exact orientation of the mirror in 2D and 3D LIDARs since the encoder is used to measure
this also have limited resolution. Another important factor
is the interaction with the target surface which can
degrade the return signal. For example, if the surface
is completely black, it might absorb most of the laser pulse. Or if it is very shiny like a mirror, the laser pulse might be scattered completely away from
the original pulse direction. In both cases, the LiDAR will typically
report a maximum range error, which could mean there is empty space
along the beam direction, or that the pulse encountered a highly absorptive or
highly reflective surface. In other words, you simply can not
tell if something is there or not, and this can be a problem for
safety if your self-driving car is relying on LiDAR alone to
detect and avoid obstacles. Finally, the speed of light actually varies depending on the material
it is traveling through. The temperature and humidity
of the air can also suddenly affect the speed of light in our
time-of-flight calculations for example. 

These factors are commonly accounted for by assuming additive zero-mean
Gaussian noise on the spherical coordinates with an empirically determined or
manually tuned covariance.  The Gaussian noise model is
particularly convenient for state estimation even if it is not
perfectly accurate in most cases. Another very important source
of error that can not be accounted for so easily
is motion distortion, which arises because the vehicle
the LiDAR is attached to is usually moving relative to
the environment it's scanning. Although the car is unlikely to be moving at an appreciable fraction
of the speed of light, it is often going to be moving at an appreciable fraction of
the rotation speed of the sensor itself, which is typically around 5-20 hertz when scanning objects at distances
of 10 to a 100 meters. This means that every single point
in a LiDAR sweep is taken from a slightly different position and
a slightly different orientation, and this can cause artifacts such as duplicate objects to
appear in the LIDAR scans. This makes it much harder for a self-driving car to
understand its environment, and correcting this motion
distortion usually requires an accurate motion model for the vehicle provided by GPS and INS for example. 

\subsection{Summary}

To recap, LIDAR sensors
measure distances by emitting pulse laser light and measuring
the time of flight of the pulse. 2D or 3D LIDAR is extend
this principle by using a mirror to sweep the laser across the environment and measure distances in many directions. We will look more closely at the point
clouds created by 2D and 3D LIDARs, and how we can use them for state estimation on-board our self-driving car.

\section{LiDAR sensor model and point clouds}
\label{lidar_and_point_clouds}

In section \ref{lidar_principles}, we talked about the basic operating principles of LiDAR, one of the most popular sensor choices for
self-driving cars. In the next two sections we will learn how we
can use the point cloud generated by LiDAR sensors to do state estimation for
our self-driving car. 

By the end of this section, you'll be able
to describe the basic point cloud data structure used to store LiDAR scans. Describe common spatial operations on
point clouds such as rotation and scaling. Use the method of least squares to
fit a plane to a point cloud in order to detect the road or other surfaces. 

To begin with, recall that a 3D LIDAR sensor returns measurements of range, elevation angle, and azimuth angle for
every point that it scans. We know how to convert these spherical
coordinates into Cartesian $x, y, z$ coordinates using
the inverse sensor model, see equation \ref{lidar_inverse_sensor_model} so we can build up a large point cloud using
all the measurements from a LiDAR scan. For some LiDAR setups, it is not uncommon for these point clouds to contain
upwards of a million points or more. 

So what can we do these massive point clouds? Let us consider an example of a point cloud
we might encounter in the real world. Let us say our LIDAR scans a nearby
tree off on the side of the road, and produces a point cloud
that looks like this. 


\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.280]{img/hardware/lidar_7.jpeg}
\end{center}
\caption{Assumed LiDAR scan.}
\label{lidar_7}
\end{figure}

We only see points on the part of the tree
that is facing us because the tree and the leaves reflect infrared light. The first question you might ask is how
do we keep track of all of these points? What kinds of data structures
should we use to work with them? One common solution is to assign
an index to each of the points, say point 1 through point $n$,
and store the $x, y, z$  coordinates of each point
as a 3 by 1 column vector. From there, you could think about storing
each of these vectors in a list, or you could stack them side by side
into a matrix that we'll call $\mathbf{P}$. Doing it this way make it easier to
work with the standard linear algebra libraries, like the Python NumPy library,
which lets us take advantage of fast matrix operations rather than
iterating over a list and treating each vector independently. So what kind of operations
are we talking about? 

There are three basic spatial
operations that are important for carrying out state estimation with point clouds. 

\begin{itemize}
\item Translation
\item Rotation
\item Scaling
\end{itemize}

We will talk about each of these in turn. When we think about spatial
operations on point clouds, our intuition might be to think in terms
of physically manipulating the point cloud while our reference frame stays fixed. But for state estimation, it's more useful to think about
things the other way around. Objects in the world mostly stay
put while the reference frame attached to the vehicle moves and observes
the world from different perspectives. 

\subsection{Basic spatial operations}

So let's think about how translating our
reference frame, say, by driving for a ten meters will affect our perception
of a single point in point cloud. 

We can start by drawing the vector from the origin of our sensor frame, $S$, to a point, $P$. Now, consider a second frame, $S^{'}$, whose origin has been translated relative
to $S$ due to motion of the vehicle, see Figure \ref{lidar_8}. 


\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.280]{img/hardware/lidar_8.jpeg}
\end{center}
\caption{Translation operation.}
\label{lidar_8}
\end{figure}

Note that the basis vectors of frame $S^{'}$ are the same as the basis vectors of frame $S$. Only the origin has moved. We can draw another vector from
the origin of $S^{'}$ to the point $P$. Immediately, we notice the resulting vector, indicated here, is just the tip to
tail sum of the other two vectors, see Figure \ref{lidar_8}. These vectors are just geometric
objects until we express them in a coordinate system. What we are after are the coordinates
of the point $P$ in frame $S^{'}$. We can get these easily by just subtracting the frame-to-frame translation vector from
the coordinates of $P$ in frame $S$. This extends easily to a batch operation
on the full point cloud by simply tiling the frame-to-frame translation
in a big matrix $\mathbf{R}$, and subtracting it from
the point cloud matrix. 

\begin{framed}
\theoremstyle{remark}
\begin{remark}{}

Depending on the language or
linear algebra library you are using, you probably will nott need to
build this $\mathbf{R}$ matrix explicitly. In Python, for example, the NumPy
library is smart enough to repeat the frame-to-frame translation
implicitly using broadcasting semantics.
\end{remark}
\end{framed}

Now, let's think about what happens if rotate our reference frame instead of translating it. Again, keep in mind that we're not
changing the physical point $P$, only our view of it. So in this case, we only have to think about one vector
from the origin of frame $S$ to $P$. What does change in this case is actually
the set of basis vectors we use to express the coordinates
of the vector $S$ to $P$. Remember that the rotation matrix $\mathbf{C}$ tells
us how to find the coordinates of a vector in a rotated frame from the coordinates
of the vector in the original frame. Thus, if we know the rotation matrix
from frame $S$ to frame $S^{'}$, all we have to do is multiply it
against the coordinates of $P$ in frame $S$ to get the coordinates
of $P$ in frame  $S^{'}$. To determine the coordinates of the entire
rotated point cloud, the operation is exactly the same, thanks to
the properties of matrix multiplication. 

\begin{equation}
\mathbf{P}_{S^{'}} = \mathbf{C}_{S^{'}S} \mathbf{P}_{S}
\end{equation}

The last spatial operation
to think about is scaling, which works very similarly to rotation. But instead of changing the direction
of the basis vectors in our coordinate system,
we're changing their lengths. Mathematically, this just means
pre-multiplying the coordinates of each point by a diagonal matrix
$\mathbf{S}$ whose non-zero elements are simply the desired scaling
factors along each dimension. Often but not always these
scaling factors are the same, and the matrix multiplication is
equivalent to multiplying by a scalar. In these cases, we say that the scaling
is isotropic or equal in every direction. We can use the same matrix multiplication
for individual points or for the entire point cloud,
just like we did for rotations.

Usually, the transformations we're interested in are a combination of translation and rotation and
sometimes scaling. For example, we are often interested
in estimating the translation and rotation that best aligns
to point clouds so that we can estimate the motion
of our self-driving car. Fortunately for us, it's easy to combine
all three operations into a single equation By first translating each vector, then rotating into the new frame,
and finally applying any scaling. Of course, this operation
extends to the batch case as well. So we have seen how to apply basic
spatial operations to point clouds. 

\subsection{Plane fitting}
\label{plane_fitting}

One of the most common and important applications of plane-fitting for self-driving cars is figuring out
where the road surface is and predicting where it is going to
be as the car continues driving. If you think back to your
high school geometry classes, you might remember
the equation of a plane in 3D. 

\begin{equation}
z = a + bx +cy
\end{equation}

This equation tells you how the height of the plane $z$ changes as you move around in the $x$ and
$y$ directions. It depends on three parameters, $a, b$, and $c$, which tells you the slope of the plane in each direction and
where the $z$ axis intersects the plane. So in our case, we have a bunch
of measurements of $x, y$ and $z$ from our LIDAR point cloud, and we
want to find values for the parameters $a, b$, and $c$ that give us the plane
of best fit through these points. To do this, we are going to use least-squares estimation. We will start by defining a measurement
error $e$ for each point in the point cloud. $e$ is just going to be the difference
between the predicted value of our dependent variable $\hat{z}$  and
the actual observed value of $z$. 


\begin{equation}
e_j = \hat{z}_j - z_j  = \hat{a} + \hat{b}x + \hat{c}y - z_j, j=1, \ldots, n
\end{equation}


We get $\hat{z}$ simply by plugging our
current guess for the parameters $\hat{a}$ , $\hat{b}$, and $\hat{c}$, and
the actual values of $x$ and $y$ in. In this case, the error, $e$, that we are
considering, is for a bumpy road surface, for example.  Note that for the moment, we are ignoring the actual
errors in the LiDAR measurements themselves, which also have an effect. 


We can stack all of these error terms into matrix form so we have a big matrix of
coefficients called $\mathbf{A}$. Multiplied by our parameter vector $\mathbf{x}$,
minus our stack measurements $\mathbf{b}$. 

\begin{equation}
\underbrace{\begin{bmatrix}
e_1 \\
e_2 \\
\vdots \\
e_n
\end{bmatrix}}_{\mathbf{e}} = 
\underbrace{\begin{bmatrix}
1 & x_1 & y_1 \\
1 & x_2 & y_2 \\
\vdots & \vdots & \vdots \\
1 & x_n & y_n
\end{bmatrix}}_{\mathbf{A}}
\underbrace{\begin{bmatrix}
a \\
b \\
c
\end{bmatrix}}_{\mathbf{x}} -
\underbrace{\begin{bmatrix}
z_1 \\
z_2 \\
\vdots \\
z_n
\end{bmatrix}}_{\mathbf{b}}
\end{equation}


You can work out the matrix multiplication yourself to see that we get back the same measurement error
equations we started out with. 

Now, all we have to do is minimize
the square of this error and we will have our solution. We can start by multiplying out
the square to get a matrix polynomial in the parameter vector $\mathbf{x}$. From there, we take the partial derivative
of the squared error function with respect to the parameter vector $\mathbf{x}$ and
set it to 0 to find the minimum. This gives us the linear system we will
need to solve to obtain the final least squares estimate. 

\begin{equation}
\hat{\mathbf{x}} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}
\end{equation}


We can solve this linear system using
an efficient numerical solver like Python NumPy's solve function. Or just use the pseudo inverse to get our
final answer for the plane parameters. 

One important thing to notice here is that we did not account for sensor noise in our $x, y, z$ measurements. All we did was to find the plane
of best fit through a set of points. It is certainly possible to set this
problem up in a more sophisticated way that does account for sensor noise. You could use a batch approach
similar to what we just discussed, or you could even think about including the
road parameters in the column filter to estimate them on the fly as
the sensor data comes in. 

The best solution for your self-driving application will depend
on how much you trust your LiDAR data and how much thought you want to give
to uncertainty in the road surface. Now, although all of the operations we've
described here can be easily implemented with NumPy or any other linear algebra library, there is a fantastic open source tool
called the Point Cloud Library, or PCL, that provides all sorts of useful
functions for working with point clouds. In fact, it is so useful that you'll
find it everywhere in industry. The core library is built with C++, but there are unofficial Python
bindings available as well. If you want to learn more
about the features PCL.

\subsection{Summary}

In summary, we have seen that point
clouds are a way of capturing all of the measurements from a LiDAR scan. They are often stored as a big matrix. We saw how we can use linear algebra to
do useful operations on point clouds, like translating, rotating, and scaling. We also saw how we can use
the least squares algorithm to fit a 3D plane to a point cloud
to find the road surface. The Point Cloud Library, or PCL,
implements a bunch of useful tools for working with point clouds in C++. One of the most useful algorithms in PCL
is called the iterative closest point algorithm, or ICP,
which is a common method for estimating the motion of a self-driving
car using two LIDAR point clouds. 


\section{Pose estimation from LiDAR data}

In this section, we will talk
about how we can actually use these operations with real point clouds to estimate the motion of
a self-driving car. The way that we do this in general is by solving something called the point set
registration problem, which is one of
the most important problems in computer vision and
pattern recognition. 

By the end of this section, you will be able to
describe the point set registration problem and how it can be used for
state estimation, describe and implement the Iterative Closest Point or ICP algorithm for
point set registration, and understand some
common pitfalls of using ICP for state estimation
on self-driving cars. 

\subsection{The point set registration problem}
\label{point_set_registration_problem}

Let us explore this problem by returning to our example of a self-driving car observing a tree from a LIDAR
mounted on the cars roof. At time $t=1$ say, the LiDAR returns
a point cloud that follows the contour of
the tree as before.  The coordinates of every point in the point cloud are given relative to the pose of the lidar at the time
of the scan, we will call this
coordinate frame $S$. At time $t=2$ the car is
driven a bit further ahead, but the LiDAR can
still see the tree and return a second point cloud whose coordinates are again
specified relative to the pose of the lidar
at time t equals two, we will call this
coordinate frame $S^{'}$. Now, the point set
registration problems says, given two point clouds in
two different coordinate frames, and with the knowledge that they correspond to or contain the same object
in the world, how shall we align them to
determine how the sensor must have moved
between the two scans? More specifically,
we want to figure out the optimal translation and the optimal rotation between the two sensor
reference frames that minimizes the distance
between the 2 point clouds. To keep things simple, we are going to pretend that
we have an ideal LiDAR sensor that measures the entire point
cloud all at once, so that we can ignore the
effects of motion distortion. Now, if we somehow knew that
each and every point in the second point
cloud corresponded to a specific point in
the first point cloud, and we knew ahead of time which points
corresponding to which, we could solve this
problem easily. All we would have to do is
find the translation and rotation that lines up
each point with its twin. 

In this example, our ideal rotation matrix would be the identity matrix, that is no rotation at all, and a ideal translation would be along the cars forward direction. The problem is
that, in general we do not know which points
correspond to each other.  Feature matching which is one way of determining correspondences between
points using camera data. For now let us think
about how we might solve this problem without knowing any of the correspondences
ahead of time. The most popular algorithm for solving this kind of
problem is called the Iterative Closest Point
algorithm or ICP for short. 

\subsection{Iterative closest point}

The basic intuition behind ICP is this, when we find the optimal
translation and rotation, or if we know something
about them in advance, and we use them to transform one point cloud into the
coordinate frame of the other, the pairs of points that truly correspond to each other will be the ones that are closest to each other in a Euclidean sense. This makes sense if you consider the simplified case where our LiDAR sensors scans exactly the same points just from two different
vantage points. In this case, there is a one-to-one mapping between points
in the two scans, and this mapping is completely determined by the motion
of the sensor. 

This is  the ideal case for when we found the best possible
translation and rotation, but what if we do not have the optimal translation
and rotation, how do we decide which points
actually go together? Well, with ICP we use
a simple heuristic, and say that for
each point in one cloud, the best candidate for
a corresponding point in the other cloud is the point that is closest to it right now. The idea here is
that this heuristic should give us
the correspondences that let us make our next guess for the translation and rotation, that is a little bit better
than our current guess. As our guesses get
better and better, our correspondences should also get better and better until we eventually converge
to the optimal motion and the optimal correspondences. 

This iterative optimization scheme using the closest point heuristic
is where ICP gets its name. It is important to
note that the word closest implies that we
will find points that lie close together after applying the frame to
frame transformation only. This is, because the vehicle is moving it is almost never the case that a laser beam will hit exactly the same
surface point twice. 

Let us now talk about the steps
in the ICP algorithm. First and most
important is that we need an initial guess
for the transformation. 

\begin{equation}
\check{\mathbf{C}_{S^{'}S}}, \check{\mathbf{r}_{S}^{S^{'}S}} 
\end{equation}


We need this because there is no guarantee that the closest point heuristic will actually converge to
the best solution. It's very easy to get stuck in a local minimum in these kinds of
iterative optimization schemes. So, we will need
a good starting guess. 

Now, the initial guess can come from a number of sources. One of the most
common sources for robotics applications
like self-driving is a motion model, which could be
supplied by an IMU or by wheel odometry or
something really simple like a constant velocity or
even a zero velocity model. How complex the motion model
needs to be to give us a good initial guess really depends on how smoothly
the car is driving. If the car is moving
slowly relative to the scanning rate
of the LIDAR sensor, one may even use the last known pose
as the initial guess. For our example, let's say we use a very noisy IMU and emotion model to provide
the initial guess. The model tells us that
the sensor translated forward a bit and
also pitched upwards. In step two, we will use
this initial guess to transform the coordinates of the points in one cloud into the reference
frame of the other, and then match each point in the second cloud to the closest
point in the first cloud. 


\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.280]{img/hardware/lidar_9.jpeg}
\end{center}
\caption{Associate each point in $\mathbf{P}_{S^{'}}$ with the nearest point in $\mathbf{P}_{S}$.}
\label{lidar_9}
\end{figure}

Figure \ref{lidar_9} shows
the associations for the first four points and
the last four points. Note that there's nothing
stopping two points in one cloud from being
associated with the same point in
the other cloud. 

Step three is to take all of those match points and find
the transformation that minimizes the sum of squared distances between
the corresponding points. You can visualize
it as wrapping an elastic band around
each pair of matched points, and then letting go and
waiting for the system to find its lowest
energy configuration. The translation and
rotation associated with this minimum energy configuration are the optimal solutions. Then, we repeat the process
using the translation and rotation we just solved for
as our new initial guess. We keep going until
the optimization converges. Thus, in the next iteration, we use the new translation
and rotation to transform the point cloud and then
find the closest matches, solve for the optimal
transformation, and keep going until we reach the optimum after a few iterations. 


How do we actually solve for the optimal transformation
in step three? One option is to use least-squares. Our goal here is to
find the rotation and translation that best aligns
the two point clouds. Specifically, you want
to minimize the sum of squared euclidean
distances between each pair of matched points, which is one of the loss function that we've defined here. 


\begin{equation}
{\check{\mathbf{C}}_{S^{'}S}, \check{\mathbf{r}}_{S}^{S^{'}S}} = argmin_{\mathbf{C}_{S^{'}S}, \mathbf{r}_{S}^{S^{'}S}} L_{LS}(\mathbf{C}_{S^{'}S}, \mathbf{r}_{S}^{S^{'}S}) 
\end{equation}

This least squares problem
is a little bit more complex. This is because
the rotation matrix is inside the loss function. It turns out the
rotation matrices do not behave like vectors. If you add two vectors together, you get another vector. But if you add two
rotation matrices together, the result is not necessarily
a valid rotation matrix. In reality, 3D rotations
belong to something called the special orthogonal group or SO3. 

It turns out that there is a nice closed form solution to this least-squares
problem which was discovered in the 1960s. We will not  go through
the derivation here, but the good news
is that there is a simple four step algorithm you can follow to
solve this problem. The first two steps are easy. First, we compute the centroid of each point cloud by taking the average over
the coordinates of each point. This is exactly like calculating the center of mass in physics. 

\begin{equation}
\boldsymbol{\mu}_S = \frac{1}{n} \sum_{j=1}^{n} \mathbf{p}_{S}^{j}, ~~ \boldsymbol{\mu}_{S^{'}} = \frac{1}{n} \sum_{j=1}^{n} \mathbf{p}_{S^{'}}^{j}
\end{equation}

Second, we work at a three-by-three matrix capturing the spread of
the two point clouds. 

\begin{equation}
\mathbf{W}_{S^{'}S} = \frac{1}{n} \sum_{j=1}^{n} (\mathbf{p}_{S}^{j} - \boldsymbol{\mu}_S) (\mathbf{p}_{S^{'}}^{j} - \boldsymbol{\mu}_{S^{'}})^T
\end{equation}

You can think of this $\mathbf{W}$ matrix
as something like an inertia matrix you might
encounter in mechanics. The $\mathbf{W}$ matrix is the quantity
we are going to use to estimate the optimal
rotation matrix. 

Step three is actually finding the optimal rotation matrix. This step is the most complex, and it involves taking
something called the singular value decomposition
or SVD of the $\mathbf{W}$ matrix. 

\begin{framed}
\theoremstyle{remark}
\begin{remark}{\textbf{Singular Value Decomposition or SVD}}

The SVD is a way of factorizing a matrix into the product
of two unitary matrices, $\mathbf{U}$ and $\mathbf{V}$, and a diagonal matrix $\mathbf{D}$, whose non-zero entries
are called the singular values of the original matrix. 

\begin{equation}
\mathbf{W}_{S^{'}S} = \mathbf{U}\mathbf{D}\mathbf{V}^T
\end{equation}
\end{remark}
\end{framed}

There are several ways to interpret the SVD. For us, it is easiest
to think about $\mathbf{U}$ and $\mathbf{V}$ as rotations and the $\mathbf{D}$ matrix
as a scaling matrix. Since we are dealing with rigid body motion in this problem, we do not want any scaling
in a rotation estimate, so we will replace
the $\mathbf{D}$ matrix with something like the identity matrix
to remove the scaling. 

\begin{equation}
\mathbf{D} = 
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & \text{det}\mathbf{U}\text{det}\mathbf{V}
\end{bmatrix}
\end{equation}

It turns out that in some cases, the SVD will give us rotation matrices that
are not proper rotations. That is, they also have a reflection about
the axis of rotation. To make sure that we
get a proper rotation, we choose the bottom
right term to be the product of
the determinants of $\mathbf{U}$ and $\mathbf{V}$. This number will be either plus one or minus one and will
cancel out any reflection. Once we have this,
all we have to do is multiply out the three matrices. This gives us back
the optimal rotation matrix. 

Now, that we have our rotation estimate, the last step is to
recover the translation. 

\begin{equation}
\hat{\mathbf{r}}_{S}^{S^{'}S} = \boldsymbol{\mu}_S - \check{\mathbf{C}}_{S^{'}S}^T \boldsymbol{\mu}_{S^{'}}, ~~ \check{\mathbf{C}}_{S^{'}S} = \mathbf{U}\mathbf{D}\mathbf{V}^T
\end{equation}

This part is very straightforward and only involves rotating the centroid of one point cloud into
the reference frame of the other and then
taking the difference of the coordinate vectors to find the translation that will best align the two point clouds. 

One other important thing to think about, both from a safety standpoint
and when we start talking about sensor fusion, is how confident we should be in the output of the ICP algorithm. There are a number
of different ways to estimate the uncertainty or the covariance of
the estimated motion parameters. An accurate and
relatively simple method is to use this equation here. 

\begin{equation}
\text{cov}(\hat{\mathbf{x}}) \approx
\begin{bmatrix}
(\frac{\partial^2 L}{\partial \mathbf{x}^2})^{-1}, \frac{\partial^2 L}{\partial \mathbf{z}\partial\mathbf{x}}, \frac{\partial^2 L}{\partial \mathbf{z}\partial\mathbf{x}}(\frac{\partial^2 L}{\partial \mathbf{x}^2})^{-1} 
\end{bmatrix}_{\mathbf{x} = \hat{\mathbf{x}}}
\end{equation}

This expression tells us
how the covariance of the estimated motion parameters is related to the covariance of the measurements in
the two point clouds using certain second-order
derivatives of the least squares cost function. While its expression is
accurate and fast to compute, it's a little tricky to derive these second-order
derivatives when there's a constraint quantity like
a rotation matrix in the mix. For our purposes, we're
generally going to hand tune the ICP covariance matrix to
give us acceptable results. 

\subsubsection{ICP variants}

You have now seen
the basic vanilla algorithm for the iterative
closest point method, but it's not the only way
of solving a problem. In fact, this algorithm is just one variant of ICP
called Point-to-point ICP, which derives its name from the fact that
our objective function or loss function
minimizes the distance between pairs of
corresponding points. 

Another popular variant that works well in unstructured
environments like cities or indoors is
called Point-to-plain ICP. Instead of minimizing
the distance between pairs of points, we fit a number of planes to the first point cloud and
then minimize the distance between each point in the second cloud and its closest plane
in the first cloud. These planes could
represent things like walls or road surfaces. The challenging part
of the algorithm is actually figuring out
where all the planes are. You can check out
the documentation for Point-to-plain ICP in the point cloud library
for more information. 

\subsection{Objects in motion}

Now, up until this point, we've been assuming
that the objects seen by our LiDAR are stationary. What
if they're moving? A common example of this is
if our car is driving in traffic down a busy highway and scanning the vehicle
in front of it. Both vehicles are traveling
at the same speed while our self-driving car is happily collecting LiDAR data points. We ask ourselves again, what motion of the car best
aligns the two point clouds? The answer we get is that
we haven't moved at all. But, of course, we did move, just not relative to the vehicle
directly ahead of us. This is obviously
a contrived example. In reality, the point
cloud would also include many stationary
objects like roads, and buildings, and trees. But naively using ICP in
the presence of moving objects will tend
to pull our motion estimates away from
the true motion. So we need to be
careful to exclude or mitigate the effects of
outlying points that violate our assumptions
of a stationary world. One way to do this is
by fusing ICP motion estimates with GPS
and INS estimates. Another option is to identify
and ignore moving objects, which we could do with some
of the computer vision. An even simpler
approach for dealing with outliers like these is to choose a different loss function that is less sensitive to large errors induced by outliers than our standard
squared error loss. The class of loss functions that have this
property are called \textbf{Robust Loss Functions} or
robust cost functions, and there are several
to choose from. We can write this
out mathematically by generalizing our least squares loss function so that
the contribution of each error is not simply
the square of its magnitude, but rather, some
other function $\rho$. Some popular choices for
robust loss functions include the Absolute
error or L1 norm, the Cauchy loss, and
the Huber loss are shown in Figure \ref{lidar_10}. 

\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.280]{img/hardware/lidar_10.jpeg}
\end{center}
\caption{Robust Loss Functions.}
\label{lidar_10}
\end{figure}

The key difference is that the slope of the
loss function does not continue to increase as
the errors become larger, but rather, it remains
constant or it tapers off. Robust loss functions make the ICP problems slightly
more difficult because we can no longer derive
a nice closed form solution for the point cloud
alignment step, and this means we need to add another iterative optimization
step inside our main loop. However, the benefits cannot
weigh the added complexity. 

\subsection{Summary}

In summary, the Iterative
Closest Point or ICP algorithm is
a way to determine the motion of
a self-driving car by aligning point clouds from
LiDAR or other sensors. ICP works by iteratively minimizing the Euclidean
distance between neighboring points
in each point cloud which is where the algorithm
gets its name. Moving objects can be
a problem for ICP since they violate the stationary
world assumption that ICP is based on. Outlier measurements like these can be mitigated with a number of techniques including
Robust Loss Functions, which assign less weight to large errors than
the usual squared error loss.

